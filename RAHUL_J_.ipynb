{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_EKvYnsyt7cE",
        "outputId": "725ebac4-3845-4348-b3cf-237c25696427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory C:\\Users\\rahul j\\Downloads\\E-Waste classification dataset",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-2741201121.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mvalidpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'C:\\Users\\rahul j\\Downloads\\E-Waste classification dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdatatrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdatatest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdatavalid\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory C:\\Users\\rahul j\\Downloads\\E-Waste classification dataset"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "\n",
        "import tensorflow as tf  # Core TensorFlow library\n",
        "\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks  # Layers, model creation, optimizers, and training callbacks\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model  # For sequential model architecture and loading saved models\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetV2B0  # Pretrained EfficientNetV2B0 model for transfer learning\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input  # Preprocessing function specific to EfficientNet\n",
        "\n",
        "import numpy as np  # Numerical operations and array handling\n",
        "\n",
        "import matplotlib.pyplot as plt  # Plotting graphs and images\n",
        "\n",
        "import seaborn as sns  # Plotting graphs and images\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Evaluation metrics for classification models\n",
        "\n",
        "import gradio as gr  # Web interface library to deploy and test ML models\n",
        "\n",
        "from PIL import Image  # For image file loading and basic image operations\n",
        "\n",
        "testpath= r'C:\\Users\\rahul j\\Downloads\\E-Waste classification dataset'\n",
        "trainpath= r'C:\\Users\\rahul j\\Downloads\\E-Waste classification dataset'\n",
        "validpath = r'C:\\Users\\rahul j\\Downloads\\E-Waste classification dataset'\n",
        "\n",
        "datatrain=tf.keras.utils.image_dataset_from_directory(trainpath,shuffle = True, image_size = (128,128), batch_size = 32, validation_split= False)\n",
        "datatest=tf.keras.utils.image_dataset_from_directory(testpath,shuffle = False, image_size = (128,128), batch_size = 32, validation_split= False)\n",
        "datavalid =tf.keras.utils.image_dataset_from_directory(validpath,shuffle = True, image_size = (128,128), batch_size = 32, validation_split= False)\n",
        "\n",
        "print(len(datatrain.class_names))\n",
        "class_names = datatrain.class_names\n",
        "print(class_names)\n",
        "\n",
        "# Set the size of the entire figure (width=10, height=10 inches)\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Take one batch from the dataset and iterate over the images and labels\n",
        "for images, labels in datatrain.take(1):\n",
        "    # Display the first 12 images from the batch\n",
        "    for i in range(12):\n",
        "        # Create a 4x3 grid of subplots and select the (i+1)th position\n",
        "        ax = plt.subplot(4, 3, i + 1)\n",
        "\n",
        "        # Display the image; convert the tensor to a NumPy array and ensure correct type\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "        # Set the title of the subplot to the class name of the image\n",
        "        plt.title(class_names[labels[i]])\n",
        "\n",
        "        # Remove axis ticks and labels for clarity\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "def plot_class_distribution(dataset, title=\"Class Distribution\"):\n",
        "    \"\"\"\n",
        "    Plots the number of items per class in a given dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: A tf.data.Dataset object created using image_dataset_from_directory\n",
        "        title: Title for the plot (e.g., 'Train Data Distribution')\n",
        "    \"\"\"\n",
        "\n",
        "    class_counts = {}  # Dictionary to hold the count of each class\n",
        "\n",
        "    # Iterate through the batches in the dataset\n",
        "    for images, labels in dataset:\n",
        "        # Convert labels tensor to numpy array and loop through each label\n",
        "        for label in labels.numpy():\n",
        "            class_name = dataset.class_names[label]  # Get class name using label index\n",
        "            # Increment the count for this class\n",
        "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "\n",
        "# Prepare data for plotting\n",
        "    class_names = list(class_counts.keys())  # List of class names\n",
        "    counts = list(class_counts.values())     # Corresponding counts for each class\n",
        "\n",
        "# Create the bar plot\n",
        "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "    plt.bar(class_names, counts, color='skyblue')  # Draw bars with class counts\n",
        "    plt.xlabel(\"Class\")  # X-axis label\n",
        "    plt.ylabel(\"Number of Items\")  # Y-axis label\n",
        "    plt.title(title)  # Plot title\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "plot_class_distribution(datatrain, \"Training Data Distribution\")\n",
        "plot_class_distribution(datavalid, \"Validation Data Distribution\")\n",
        "plot_class_distribution(datatest, \"Test Data Distribution\")\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "base_model = tf.keras.applications.EfficientNetV2B0(\n",
        "    input_shape=(128, 128, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(128, 128, 3)),\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['Accuracy'])\n",
        "\n",
        "\n",
        "# Define an EarlyStopping callback to stop training when validation loss stops improving\n",
        "early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',            # Metric to monitor (validation loss here)\n",
        "    patience=3,                   # Number of epochs to wait after last improvement before stopping\n",
        "    restore_best_weights=True     # After stopping, restore the model weights from the epoch with the best val_loss\n",
        ")\n",
        "\n",
        "# Set the number of epochs to train the model\n",
        "epochs = 15\n",
        "\n",
        "# Train the model on the training dataset 'datatrain'\n",
        "history = model.fit(\n",
        "    datatrain,                      # Training data generator or dataset\n",
        "    validation_data=datavalid,      # Validation data generator or dataset\n",
        "    epochs=epochs,                  # Number of training epochs\n",
        "    batch_size=100,                 # Number of samples per gradient update\n",
        "    callbacks=[early]               # List of callbacks to apply during training (e.g., early stopping)\n",
        ")\n",
        "\n",
        "model.summary() # Print the architecture summary of the  model\n",
        "base_model.summary() # Print the architecture summary of the base model\n",
        "\n",
        "\n",
        "### Plotting Training and Validation Accuracy and Loss Over Epochs\n",
        "\n",
        "acc = history.history['Accuracy']           # Training accuracy\n",
        "val_acc = history.history['val_Accuracy']   # Validation accuracy\n",
        "loss = history.history['loss']              # Training loss\n",
        "val_loss = history.history['val_loss']      # Validation loss\n",
        "\n",
        "epochs_range = range(len(acc))              # X-axis range based on number of epochs\n",
        "\n",
        "plt.figure(figsize=(10, 8))                 # Set overall figure size\n",
        "\n",
        "plt.subplot(1, 2, 1)                        # 1 row, 2 columns, position 1\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')       # Plot training accuracy\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy') # Plot validation accuracy\n",
        "plt.legend(loc='lower right')              # Show legend at lower right\n",
        "plt.title('Training vs Validation Accuracy') # Set title for accuracy plot\n",
        "\n",
        "plt.subplot(1, 2, 2)                        # 1 row, 2 columns, position 2\n",
        "plt.plot(epochs_range, loss, label='Training Loss')          # Plot training loss\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')    # Plot validation loss\n",
        "plt.legend(loc='upper right')              # Show legend at upper right\n",
        "plt.title('Training vs Validation Loss')    # Set title for loss plot\n",
        "\n",
        "plt.show()                                  # Display the plots\n",
        "\n",
        "loss, accuracy = model.evaluate(datatest)\n",
        "print(f'Test accuracy is{accuracy:.4f}, Test loss is {loss:.4f}')\n",
        "\n",
        "\n",
        "### Evaluate Model Performance on Test Data using Confusion Matrix and Classification Report\n",
        "\n",
        "# Extract true labels from all batches\n",
        "y_true = np.concatenate([y.numpy() for x, y in datatest], axis=0)  # Ground truth labels\n",
        "\n",
        "# Get predictions as probabilities and then predicted classes\n",
        "y_pred_probs = model.predict(datatest)\n",
        "\n",
        "# Class with highest probability\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Print confusion matrix and classification report\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "### Plot Confusion Matrix as Heatmap for Better Visualization\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)                                     # Compute confusion matrix\n",
        "                                                     # Import seaborn for visualization\n",
        "\n",
        "plt.figure(figsize=(10, 8))                                               # Set figure size\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            cmap='Blues')                                                 # Create heatmap with class labels\n",
        "\n",
        "plt.xlabel('Predicted')                                                   # Label for x-axis\n",
        "plt.ylabel('True')                                                        # Label for y-axis\n",
        "plt.title('Confusion Matrix')                                             # Title for the plot\n",
        "plt.show()                                                                # Display the plot\n",
        "\n",
        "### Display Sample Predictions: True Labels vs Predicted Labels\n",
        "\n",
        "class_names = datatest.class_names                                           # Get class names from test dataset\n",
        "\n",
        "for images, labels in datatest.take(1):                                     # Take one batch from test data\n",
        "    predictions = model.predict(images)                                     # Predict class probabilities\n",
        "    pred_labels = tf.argmax(predictions, axis=1)                            # Get predicted class indices\n",
        "\n",
        "    for i in range(8):                                                      # Display first 8 images from batch\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))                       # Convert tensor to image\n",
        "        plt.title(f\"True: {class_names[labels[i]]}, Pred: {class_names[pred_labels[i]]}\")  # Title with labels\n",
        "        plt.axis(\"off\")                                                     # Hide axes\n",
        "        plt.show()                                                          # Show image\n",
        "\n",
        "# Save model in Keras format with architecture, weights, and training configuration\n",
        "model.save('Efficient_classify.keras')\n",
        "\n",
        "# Define your class labels\n",
        "class_names = ['Battery', 'Keyboard', 'Microwave', 'Mobile', 'Mouse', 'PCB', 'Player', 'Printer', 'Television', 'Washing Machine']\n",
        "\n",
        "# Load your Keras model\n",
        "model = tf.keras.models.load_model('Efficient_classify.keras')\n",
        "\n"
      ]
    }
  ]
}